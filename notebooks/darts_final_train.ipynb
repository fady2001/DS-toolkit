{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a61fdbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac17f0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000888, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/raw/train.csv', parse_dates=['date'])\n",
    "holiday_df = pd.read_csv('../data/raw/holidays_events.csv', parse_dates=['date'])\n",
    "oil_df = pd.read_csv('../data/raw/oil.csv', parse_dates=['date'])\n",
    "stores_df = pd.read_csv('../data/raw/stores.csv')\n",
    "transactions_df = pd.read_csv('../data/raw/transactions.csv', parse_dates=['date'])\n",
    "test_df = pd.read_csv('../data/raw/test.csv', parse_dates=['date'])\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5de525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 6), No null rows in train_df.\n",
      "\n",
      "shape:(350, 6), No null rows in holiday_df.\n",
      "\n",
      "shape:(1218, 2), Null rows in oil_df:\n",
      "           date  dcoilwtico\n",
      "0    2013-01-01         NaN\n",
      "14   2013-01-21         NaN\n",
      "34   2013-02-18         NaN\n",
      "63   2013-03-29         NaN\n",
      "104  2013-05-27         NaN\n",
      "132  2013-07-04         NaN\n",
      "174  2013-09-02         NaN\n",
      "237  2013-11-28         NaN\n",
      "256  2013-12-25         NaN\n",
      "261  2014-01-01         NaN\n",
      "274  2014-01-20         NaN\n",
      "294  2014-02-17         NaN\n",
      "338  2014-04-18         NaN\n",
      "364  2014-05-26         NaN\n",
      "393  2014-07-04         NaN\n",
      "434  2014-09-01         NaN\n",
      "497  2014-11-27         NaN\n",
      "517  2014-12-25         NaN\n",
      "522  2015-01-01         NaN\n",
      "534  2015-01-19         NaN\n",
      "554  2015-02-16         NaN\n",
      "588  2015-04-03         NaN\n",
      "624  2015-05-25         NaN\n",
      "653  2015-07-03         NaN\n",
      "699  2015-09-07         NaN\n",
      "757  2015-11-26         NaN\n",
      "778  2015-12-25         NaN\n",
      "783  2016-01-01         NaN\n",
      "794  2016-01-18         NaN\n",
      "814  2016-02-15         NaN\n",
      "843  2016-03-25         NaN\n",
      "889  2016-05-30         NaN\n",
      "914  2016-07-04         NaN\n",
      "959  2016-09-05         NaN\n",
      "1017 2016-11-24         NaN\n",
      "1039 2016-12-26         NaN\n",
      "1044 2017-01-02         NaN\n",
      "1054 2017-01-16         NaN\n",
      "1079 2017-02-20         NaN\n",
      "1118 2017-04-14         NaN\n",
      "1149 2017-05-29         NaN\n",
      "1174 2017-07-03         NaN\n",
      "1175 2017-07-04         NaN\n",
      "\n",
      "shape:(54, 5), No null rows in stores_df.\n",
      "\n",
      "shape:(83488, 3), No null rows in transactions_df.\n",
      "\n",
      "shape:(28512, 5), No null rows in test_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print null rows of any dataframe\n",
    "def print_null_rows(df, name):\n",
    "    null_rows = df[df.isnull().any(axis=1)]\n",
    "    if not null_rows.empty:\n",
    "        print(f\"shape:{df.shape}, Null rows in {name}:\\n{null_rows}\\n\")\n",
    "    else:\n",
    "        print(f\"shape:{df.shape}, No null rows in {name}.\\n\")\n",
    "    \n",
    "print_null_rows(train_df, 'train_df')\n",
    "print_null_rows(holiday_df, 'holiday_df')\n",
    "print_null_rows(oil_df, 'oil_df')\n",
    "print_null_rows(stores_df, 'stores_df')\n",
    "print_null_rows(transactions_df, 'transactions_df')\n",
    "print_null_rows(test_df, 'test_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a29a7",
   "metadata": {},
   "source": [
    "## 1. Darts Final Train without lagging and all date index must exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77c8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate missing values in oil prices\n",
    "all_dates = pd.date_range(start=oil_df['date'].min(), end=oil_df['date'].max())\n",
    "oil_df = oil_df.set_index('date').reindex(all_dates).rename_axis('date').reset_index()\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].interpolate(method='polynomial', order=2)\n",
    "# fill backward and forward fill for oil prices\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d599f7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 6), No null rows in train_df.\n",
      "\n",
      "shape:(350, 6), No null rows in holiday_df.\n",
      "\n",
      "shape:(1704, 2), No null rows in oil_df.\n",
      "\n",
      "shape:(54, 5), No null rows in stores_df.\n",
      "\n",
      "shape:(83488, 3), No null rows in transactions_df.\n",
      "\n",
      "shape:(28512, 5), No null rows in test_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_null_rows(train_df, 'train_df')\n",
    "print_null_rows(holiday_df, 'holiday_df')\n",
    "print_null_rows(oil_df, 'oil_df')\n",
    "print_null_rows(stores_df, 'stores_df')\n",
    "print_null_rows(transactions_df, 'transactions_df')\n",
    "print_null_rows(test_df, 'test_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6203a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create faetures for train and test sets\n",
    "def create_features(df):\n",
    "    def days_since_payday(date):\n",
    "        day = date.day\n",
    "        if day <= 15:\n",
    "            # Days since last month's end\n",
    "            last_month_end = date.replace(day=1) - timedelta(days=1)\n",
    "            return (date - last_month_end).days\n",
    "        else:\n",
    "            # Days since 15th of current month\n",
    "            current_month_15th = date.replace(day=15)\n",
    "            return (date - current_month_15th).days\n",
    "        \n",
    "    def days_until_payday(date):\n",
    "        day = date.day\n",
    "        if day < 15:\n",
    "            # Days until 15th\n",
    "            return 15 - day\n",
    "        else:\n",
    "            # Days until month end\n",
    "            next_month = date.replace(day=28) + timedelta(days=4)\n",
    "            month_end = next_month - timedelta(days=next_month.day)\n",
    "            return (month_end - date).days\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
    "    df['is_payday'] = ((df['day'] == 15) | df['date'].dt.is_month_end).astype(int)\n",
    "    df['days_since_payday'] = df['date'].apply(days_since_payday)\n",
    "    df['days_until_payday'] = df['date'].apply(days_until_payday)\n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, target_col='sales', lags=[1, 7, 14, 30]):\n",
    "    \"\"\"Create lag features for time series\"\"\"\n",
    "    df_sorted = df.sort_values(['store_nbr', 'family', 'date'])\n",
    "    \n",
    "    for lag in lags:\n",
    "        df_sorted[f'{target_col}_lag_{lag}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].shift(lag)\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "def create_rolling_features(df, target_col='sales', windows=[7, 14, 30]):\n",
    "    \"\"\"Create rolling window statistics\"\"\"\n",
    "    df_sorted = df.sort_values(['store_nbr', 'family', 'date'])\n",
    "    \n",
    "    for window in windows:\n",
    "        # Rolling mean\n",
    "        df_sorted[f'{target_col}_rolling_mean_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Rolling std\n",
    "        df_sorted[f'{target_col}_rolling_std_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "        \n",
    "        # Rolling max\n",
    "        df_sorted[f'{target_col}_rolling_max_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "        )\n",
    "        \n",
    "        # Rolling min\n",
    "        df_sorted[f'{target_col}_rolling_min_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "        )\n",
    "    \n",
    "    return df_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af9dfb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 20), No null rows in train_df_temporal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = create_features(train_df)\n",
    "print_null_rows(train_df, 'train_df_temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ed0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_rolling_features(train_df)\n",
    "# print number of nulls in train_df_rolling\n",
    "# fill nulls in rolling features with 0\n",
    "print_null_rows(train_df, 'train_df_oil')\n",
    "train_df.fillna(0, inplace=True)\n",
    "print_null_rows(train_df, 'train_df_oil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "289d9e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' No lagging features because Darts Model handle them internally.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" No lagging features because Darts Model handle them internally.\"\"\"\n",
    "# train_df = create_lag_features(train_df)\n",
    "# train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7612d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "national_holidays = holiday_df[holiday_df['locale'] == 'National']['date'].unique()\n",
    "regional_holidays = holiday_df[holiday_df['locale'] == 'Regional']['date'].unique()\n",
    "local_holidays = holiday_df[holiday_df['locale'] == 'Local']['date'].unique()\n",
    "additional_holidays = holiday_df[holiday_df['type'] == 'Additional']['date'].unique()\n",
    "working_days = holiday_df[holiday_df['type'] == 'Work Day']['date'].unique()\n",
    "events = holiday_df[holiday_df['type'] == 'Event']['date'].unique()\n",
    "bridge_days = holiday_df[holiday_df['type'] == 'Bridge']['date'].unique()\n",
    "transsferred_days = holiday_df[holiday_df['transferred'] == True]['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "991f35ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 40), No null rows in train_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add holiday features to train\n",
    "def add_holiday_features(df):\n",
    "    df['is_national_holiday'] = df['date'].isin(national_holidays).astype(int)\n",
    "    df['is_regional_holiday'] = df['date'].isin(regional_holidays).astype(int)\n",
    "    df['is_local_holiday'] = df['date'].isin(local_holidays).astype(int)\n",
    "    df['is_additional_holiday'] = df['date'].isin(additional_holidays).astype(int)\n",
    "    df['is_working_day'] = df['date'].isin(working_days).astype(int)\n",
    "    df['is_event'] = df['date'].isin(events).astype(int)\n",
    "    df['is_bridge_day'] = df['date'].isin(bridge_days).astype(int)\n",
    "    df['is_transferred_day'] = df['date'].isin(transsferred_days).astype(int)\n",
    "    return df\n",
    "\n",
    "# create features for train and test sets\n",
    "train_df = add_holiday_features(train_df)\n",
    "print_null_rows(train_df, 'train_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95d036fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 41), No null rows in train_df_final.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.merge(oil_df, on='date', how='left')\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fad06ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 45), No null rows in train_df_final.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.merge(stores_df, on='store_nbr', how='left')\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e799dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 46), Null rows in train_df_final:\n",
      "              id       date  store_nbr      family  sales  onpromotion  year  \\\n",
      "0              0 2013-01-01          1  AUTOMOTIVE    0.0            0  2013   \n",
      "364       648648 2014-01-01          1  AUTOMOTIVE    0.0            0  2014   \n",
      "728      1297296 2015-01-01          1  AUTOMOTIVE    0.0            0  2015   \n",
      "915      1630530 2015-07-07          1  AUTOMOTIVE    0.0            0  2015   \n",
      "1092     1945944 2016-01-01          1  AUTOMOTIVE    0.0            0  2016   \n",
      "...          ...        ...        ...         ...    ...          ...   ...   \n",
      "2999932  1298945 2015-01-01         54     SEAFOOD    0.0            0  2015   \n",
      "3000296  1947593 2016-01-01         54     SEAFOOD    0.0            0  2016   \n",
      "3000298  1951157 2016-01-03         54     SEAFOOD    2.0            0  2016   \n",
      "3000299  1952939 2016-01-04         54     SEAFOOD    3.0            0  2016   \n",
      "3000661  2598023 2017-01-01         54     SEAFOOD    0.0            0  2017   \n",
      "\n",
      "         month  day  dayofweek  ...  is_working_day  is_event  is_bridge_day  \\\n",
      "0            1    1          1  ...               0         0              0   \n",
      "364          1    1          2  ...               0         0              0   \n",
      "728          1    1          3  ...               0         0              0   \n",
      "915          7    7          1  ...               0         0              0   \n",
      "1092         1    1          4  ...               0         0              0   \n",
      "...        ...  ...        ...  ...             ...       ...            ...   \n",
      "2999932      1    1          3  ...               0         0              0   \n",
      "3000296      1    1          4  ...               0         0              0   \n",
      "3000298      1    3          6  ...               0         0              0   \n",
      "3000299      1    4          0  ...               0         0              0   \n",
      "3000661      1    1          6  ...               0         0              0   \n",
      "\n",
      "         is_transferred_day  dcoilwtico       city      state  type  cluster  \\\n",
      "0                         0   93.140000      Quito  Pichincha     D       13   \n",
      "364                       0   96.809553      Quito  Pichincha     D       13   \n",
      "728                       0   52.981201      Quito  Pichincha     D       13   \n",
      "915                       0   52.330000      Quito  Pichincha     D       13   \n",
      "1092                      0   37.633604      Quito  Pichincha     D       13   \n",
      "...                     ...         ...        ...        ...   ...      ...   \n",
      "2999932                   0   52.981201  El Carmen     Manabi     C        3   \n",
      "3000296                   0   37.633604  El Carmen     Manabi     C        3   \n",
      "3000298                   0   37.290982  El Carmen     Manabi     C        3   \n",
      "3000299                   0   36.810000  El Carmen     Manabi     C        3   \n",
      "3000661                   1   52.655576  El Carmen     Manabi     C        3   \n",
      "\n",
      "         transactions  \n",
      "0                 NaN  \n",
      "364               NaN  \n",
      "728               NaN  \n",
      "915               NaN  \n",
      "1092              NaN  \n",
      "...               ...  \n",
      "2999932           NaN  \n",
      "3000296           NaN  \n",
      "3000298           NaN  \n",
      "3000299           NaN  \n",
      "3000661           NaN  \n",
      "\n",
      "[245784 rows x 46 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.merge(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93326774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 46), No null rows in train_df_final.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill missing values in transactions with 0\n",
    "train_df['transactions'] = train_df['transactions'].fillna(0)\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f11d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../data/interim/Darts_final_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e87ae8",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering for Test Set (Avoiding Data Leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1119184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28512, 19), No null rows in test_df_temporal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of test_df for processing\n",
    "test_processed = test_df.copy()\n",
    "\n",
    "# Apply temporal features (safe - no data leakage)\n",
    "test_processed = create_features(test_processed)\n",
    "print_null_rows(test_processed, 'test_df_temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9665582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28512, 27), No null rows in test_df_holidays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add holiday features (safe - holidays are predetermined)\n",
    "test_processed = add_holiday_features(test_processed)\n",
    "print_null_rows(test_processed, 'test_df_holidays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8ad7200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28512, 28), No null rows in test_df_oil.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge oil prices with test data\n",
    "test_processed = test_processed.merge(oil_df, on='date', how='left')\n",
    "print_null_rows(test_processed, 'test_df_oil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1539f24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28512, 32), No null rows in test_df_stores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge store information\n",
    "test_processed = test_processed.merge(stores_df, on='store_nbr', how='left')\n",
    "print_null_rows(test_processed, 'test_df_stores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63ba6a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28512, 33), No null rows in test_df_transactions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For transactions in test period, we need to be careful\n",
    "# We can either:\n",
    "# 1. Use 0 for all test transactions (conservative)\n",
    "# 2. Use historical averages for each store\n",
    "# 3. Predict transactions separately\n",
    "\n",
    "# Option 2: Use historical averages\n",
    "store_avg_transactions = transactions_df.groupby('store_nbr')['transactions'].mean().reset_index()\n",
    "store_avg_transactions.columns = ['store_nbr', 'avg_transactions']\n",
    "\n",
    "# Merge with test data\n",
    "test_processed = test_processed.merge(store_avg_transactions, on='store_nbr', how='left')\n",
    "\n",
    "# Use average transactions for test period\n",
    "test_processed['transactions'] = test_processed['avg_transactions']\n",
    "test_processed.drop('avg_transactions', axis=1, inplace=True)\n",
    "\n",
    "print_null_rows(test_processed, 'test_df_transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "529d2974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating rolling features for test data using only historical training data...\n"
     ]
    }
   ],
   "source": [
    "# For rolling features, we need to be very careful to avoid data leakage\n",
    "# We can only compute rolling features using historical training data\n",
    "# \n",
    "# Create rolling features using only training data up to each test date\n",
    "def create_rolling_features_no_leakage(train_df, test_df, target_col='sales', windows=[7, 14, 30]):\n",
    "    \"\"\"Create rolling features for test set using only historical training data\"\"\"\n",
    "    \n",
    "    # Combine train and test for proper sorting, but only compute rolling on train\n",
    "    # Add a temporary sales column to test (will be removed)\n",
    "    test_with_temp_sales = test_df.copy()\n",
    "    test_with_temp_sales[target_col] = 0  # Temporary placeholder\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined = pd.concat([train_df, test_with_temp_sales], ignore_index=True)\n",
    "    combined = combined.sort_values(['store_nbr', 'family', 'date'])\n",
    "    \n",
    "    # Mark train vs test\n",
    "    combined['is_train'] = combined['date'] <= train_df['date'].max()\n",
    "    \n",
    "    for window in windows:\n",
    "        # Rolling mean - only computed on training data\n",
    "        combined[f'{target_col}_rolling_mean_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
    "            lambda group: group[target_col].where(group['is_train']).rolling(window=window, min_periods=1).mean().ffill()\n",
    "        ).reset_index(level=[0, 1], drop=True)\n",
    "        \n",
    "        # Rolling std\n",
    "        combined[f'{target_col}_rolling_std_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
    "            lambda group: group[target_col].where(group['is_train']).rolling(window=window, min_periods=1).std().ffill()\n",
    "        ).reset_index(level=[0, 1], drop=True)\n",
    "        \n",
    "        # Rolling max\n",
    "        combined[f'{target_col}_rolling_max_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
    "            lambda group: group[target_col].where(group['is_train']).rolling(window=window, min_periods=1).max().ffill()\n",
    "        ).reset_index(level=[0, 1], drop=True)\n",
    "        \n",
    "        # Rolling min\n",
    "        combined[f'{target_col}_rolling_min_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
    "            lambda group: group[target_col].where(group['is_train']).rolling(window=window, min_periods=1).min().ffill()\n",
    "        ).reset_index(level=[0, 1], drop=True)\n",
    "    \n",
    "    # Return only test portion\n",
    "    test_result = combined[~combined['is_train']].copy()\n",
    "    test_result = test_result.drop(['is_train', target_col], axis=1)  # Remove temporary columns\n",
    "    \n",
    "    return test_result\n",
    "\n",
    "print(\"Creating rolling features for test data using only historical training data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f314845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_mean_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_std_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_std_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_max_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_max_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_min_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_min_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_mean_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_mean_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_std_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_max_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_min_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_mean_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_std_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_max_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_3192\\2004204811.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined[f'{target_col}_rolling_min_{window}'] = combined.groupby(['store_nbr', 'family']).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28512, 45), No null rows in test_df_final.\n",
      "\n",
      "Test processed shape: (28512, 45)\n",
      "Test processed columns: ['id', 'date', 'store_nbr', 'family', 'onpromotion', 'year', 'month', 'day', 'dayofweek', 'weekofyear', 'day_of_year', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end', 'is_payday', 'days_since_payday', 'days_until_payday', 'sales_rolling_mean_7', 'sales_rolling_std_7', 'sales_rolling_max_7', 'sales_rolling_min_7', 'sales_rolling_mean_14', 'sales_rolling_std_14', 'sales_rolling_max_14', 'sales_rolling_min_14', 'sales_rolling_mean_30', 'sales_rolling_std_30', 'sales_rolling_max_30', 'sales_rolling_min_30', 'is_national_holiday', 'is_regional_holiday', 'is_local_holiday', 'is_additional_holiday', 'is_working_day', 'is_event', 'is_bridge_day', 'is_transferred_day', 'dcoilwtico', 'city', 'state', 'type', 'cluster', 'transactions']\n"
     ]
    }
   ],
   "source": [
    "# Apply rolling features without data leakage\n",
    "test_processed = create_rolling_features_no_leakage(train_df, test_processed)\n",
    "\n",
    "# Fill any remaining NaN values with 0\n",
    "test_processed = test_processed.fillna(0)\n",
    "\n",
    "print_null_rows(test_processed, 'test_df_final')\n",
    "print(f\"Test processed shape: {test_processed.shape}\")\n",
    "print(f\"Test processed columns: {list(test_processed.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4f48606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data with features saved to '../data/interim/Darts_final_test.csv'\n",
      "\n",
      "Column consistency check:\n",
      "Missing in test: set()\n",
      "Extra in test: set()\n",
      "✅ Perfect column alignment between train and test datasets!\n"
     ]
    }
   ],
   "source": [
    "# Save the processed test data\n",
    "test_processed.to_csv('../data/interim/Darts_final_test.csv', index=False)\n",
    "print(\"Test data with features saved to '../data/interim/Darts_final_test.csv'\")\n",
    "\n",
    "# Verify column consistency between train and test\n",
    "train_cols = set(train_df.columns) - {'sales'}  # Remove target column\n",
    "test_cols = set(test_processed.columns)\n",
    "\n",
    "missing_in_test = train_cols - test_cols\n",
    "extra_in_test = test_cols - train_cols\n",
    "\n",
    "print(f\"\\nColumn consistency check:\")\n",
    "print(f\"Missing in test: {missing_in_test}\")\n",
    "print(f\"Extra in test: {extra_in_test}\")\n",
    "\n",
    "if len(missing_in_test) == 0 and len(extra_in_test) == 0:\n",
    "    print(\"✅ Perfect column alignment between train and test datasets!\")\n",
    "else:\n",
    "    print(\"⚠️  Column mismatch detected. Please review.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
