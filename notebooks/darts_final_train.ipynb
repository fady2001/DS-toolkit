{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a61fdbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac17f0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000888, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/raw/train.csv', parse_dates=['date'])\n",
    "holiday_df = pd.read_csv('../data/raw/holidays_events.csv', parse_dates=['date'])\n",
    "oil_df = pd.read_csv('../data/raw/oil.csv', parse_dates=['date'])\n",
    "stores_df = pd.read_csv('../data/raw/stores.csv')\n",
    "transactions_df = pd.read_csv('../data/raw/transactions.csv', parse_dates=['date'])\n",
    "test_df = pd.read_csv('../data/raw/test.csv', parse_dates=['date'])\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5de525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 6), No null rows in train_df.\n",
      "\n",
      "shape:(350, 6), No null rows in holiday_df.\n",
      "\n",
      "shape:(1218, 2), Null rows in oil_df:\n",
      "           date  dcoilwtico\n",
      "0    2013-01-01         NaN\n",
      "14   2013-01-21         NaN\n",
      "34   2013-02-18         NaN\n",
      "63   2013-03-29         NaN\n",
      "104  2013-05-27         NaN\n",
      "132  2013-07-04         NaN\n",
      "174  2013-09-02         NaN\n",
      "237  2013-11-28         NaN\n",
      "256  2013-12-25         NaN\n",
      "261  2014-01-01         NaN\n",
      "274  2014-01-20         NaN\n",
      "294  2014-02-17         NaN\n",
      "338  2014-04-18         NaN\n",
      "364  2014-05-26         NaN\n",
      "393  2014-07-04         NaN\n",
      "434  2014-09-01         NaN\n",
      "497  2014-11-27         NaN\n",
      "517  2014-12-25         NaN\n",
      "522  2015-01-01         NaN\n",
      "534  2015-01-19         NaN\n",
      "554  2015-02-16         NaN\n",
      "588  2015-04-03         NaN\n",
      "624  2015-05-25         NaN\n",
      "653  2015-07-03         NaN\n",
      "699  2015-09-07         NaN\n",
      "757  2015-11-26         NaN\n",
      "778  2015-12-25         NaN\n",
      "783  2016-01-01         NaN\n",
      "794  2016-01-18         NaN\n",
      "814  2016-02-15         NaN\n",
      "843  2016-03-25         NaN\n",
      "889  2016-05-30         NaN\n",
      "914  2016-07-04         NaN\n",
      "959  2016-09-05         NaN\n",
      "1017 2016-11-24         NaN\n",
      "1039 2016-12-26         NaN\n",
      "1044 2017-01-02         NaN\n",
      "1054 2017-01-16         NaN\n",
      "1079 2017-02-20         NaN\n",
      "1118 2017-04-14         NaN\n",
      "1149 2017-05-29         NaN\n",
      "1174 2017-07-03         NaN\n",
      "1175 2017-07-04         NaN\n",
      "\n",
      "shape:(54, 5), No null rows in stores_df.\n",
      "\n",
      "shape:(83488, 3), No null rows in transactions_df.\n",
      "\n",
      "shape:(28512, 5), No null rows in test_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print null rows of any dataframe\n",
    "def print_null_rows(df, name):\n",
    "    null_rows = df[df.isnull().any(axis=1)]\n",
    "    if not null_rows.empty:\n",
    "        print(f\"shape:{df.shape}, Null rows in {name}:\\n{null_rows}\\n\")\n",
    "    else:\n",
    "        print(f\"shape:{df.shape}, No null rows in {name}.\\n\")\n",
    "    \n",
    "print_null_rows(train_df, 'train_df')\n",
    "print_null_rows(holiday_df, 'holiday_df')\n",
    "print_null_rows(oil_df, 'oil_df')\n",
    "print_null_rows(stores_df, 'stores_df')\n",
    "print_null_rows(transactions_df, 'transactions_df')\n",
    "print_null_rows(test_df, 'test_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a29a7",
   "metadata": {},
   "source": [
    "## 1. Darts Final Train without lagging and all date index must exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77c8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate missing values in oil prices\n",
    "all_dates = pd.date_range(start=oil_df['date'].min(), end=oil_df['date'].max())\n",
    "oil_df = oil_df.set_index('date').reindex(all_dates).rename_axis('date').reset_index()\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].interpolate(method='polynomial', order=2)\n",
    "# fill backward and forward fill for oil prices\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d599f7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 6), No null rows in train_df.\n",
      "\n",
      "shape:(350, 6), No null rows in holiday_df.\n",
      "\n",
      "shape:(1704, 2), No null rows in oil_df.\n",
      "\n",
      "shape:(54, 5), No null rows in stores_df.\n",
      "\n",
      "shape:(83488, 3), No null rows in transactions_df.\n",
      "\n",
      "shape:(28512, 5), No null rows in test_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_null_rows(train_df, 'train_df')\n",
    "print_null_rows(holiday_df, 'holiday_df')\n",
    "print_null_rows(oil_df, 'oil_df')\n",
    "print_null_rows(stores_df, 'stores_df')\n",
    "print_null_rows(transactions_df, 'transactions_df')\n",
    "print_null_rows(test_df, 'test_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6203a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create faetures for train and test sets\n",
    "def create_features(df):\n",
    "    def days_since_payday(date):\n",
    "        day = date.day\n",
    "        if day <= 15:\n",
    "            # Days since last month's end\n",
    "            last_month_end = date.replace(day=1) - timedelta(days=1)\n",
    "            return (date - last_month_end).days\n",
    "        else:\n",
    "            # Days since 15th of current month\n",
    "            current_month_15th = date.replace(day=15)\n",
    "            return (date - current_month_15th).days\n",
    "        \n",
    "    def days_until_payday(date):\n",
    "        day = date.day\n",
    "        if day < 15:\n",
    "            # Days until 15th\n",
    "            return 15 - day\n",
    "        else:\n",
    "            # Days until month end\n",
    "            next_month = date.replace(day=28) + timedelta(days=4)\n",
    "            month_end = next_month - timedelta(days=next_month.day)\n",
    "            return (month_end - date).days\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
    "    df['is_payday'] = ((df['day'] == 15) | df['date'].dt.is_month_end).astype(int)\n",
    "    df['days_since_payday'] = df['date'].apply(days_since_payday)\n",
    "    df['days_until_payday'] = df['date'].apply(days_until_payday)\n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, target_col='sales', lags=[1, 7, 14, 30]):\n",
    "    \"\"\"Create lag features for time series\"\"\"\n",
    "    df_sorted = df.sort_values(['store_nbr', 'family', 'date'])\n",
    "    \n",
    "    for lag in lags:\n",
    "        df_sorted[f'{target_col}_lag_{lag}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].shift(lag)\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "def create_rolling_features(df, target_col='sales', windows=[7, 14, 30]):\n",
    "    \"\"\"Create rolling window statistics\"\"\"\n",
    "    df_sorted = df.sort_values(['store_nbr', 'family', 'date'])\n",
    "    \n",
    "    for window in windows:\n",
    "        # Rolling mean\n",
    "        df_sorted[f'{target_col}_rolling_mean_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Rolling std\n",
    "        df_sorted[f'{target_col}_rolling_std_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "        \n",
    "        # Rolling max\n",
    "        df_sorted[f'{target_col}_rolling_max_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "        )\n",
    "        \n",
    "        # Rolling min\n",
    "        df_sorted[f'{target_col}_rolling_min_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "        )\n",
    "    \n",
    "    return df_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af9dfb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 20), No null rows in train_df_temporal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = create_features(train_df)\n",
    "print_null_rows(train_df, 'train_df_temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_rolling_features(train_df)\n",
    "# print number of nulls in train_df_rolling\n",
    "# fill nulls in rolling features with 0\n",
    "print_null_rows(train_df, 'train_df_oil')\n",
    "train_df.fillna(0, inplace=True)\n",
    "print_null_rows(train_df, 'train_df_oil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "289d9e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' No lagging features because Darts Model handle them internally.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" No lagging features because Darts Model handle them internally.\"\"\"\n",
    "# train_df = create_lag_features(train_df)\n",
    "# train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7612d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "national_holidays = holiday_df[holiday_df['locale'] == 'National']['date'].unique()\n",
    "regional_holidays = holiday_df[holiday_df['locale'] == 'Regional']['date'].unique()\n",
    "local_holidays = holiday_df[holiday_df['locale'] == 'Local']['date'].unique()\n",
    "additional_holidays = holiday_df[holiday_df['type'] == 'Additional']['date'].unique()\n",
    "working_days = holiday_df[holiday_df['type'] == 'Work Day']['date'].unique()\n",
    "events = holiday_df[holiday_df['type'] == 'Event']['date'].unique()\n",
    "bridge_days = holiday_df[holiday_df['type'] == 'Bridge']['date'].unique()\n",
    "transsferred_days = holiday_df[holiday_df['transferred'] == True]['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "991f35ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 40), No null rows in train_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add holiday features to train\n",
    "def add_holiday_features(df):\n",
    "    df['is_national_holiday'] = df['date'].isin(national_holidays).astype(int)\n",
    "    df['is_regional_holiday'] = df['date'].isin(regional_holidays).astype(int)\n",
    "    df['is_local_holiday'] = df['date'].isin(local_holidays).astype(int)\n",
    "    df['is_additional_holiday'] = df['date'].isin(additional_holidays).astype(int)\n",
    "    df['is_working_day'] = df['date'].isin(working_days).astype(int)\n",
    "    df['is_event'] = df['date'].isin(events).astype(int)\n",
    "    df['is_bridge_day'] = df['date'].isin(bridge_days).astype(int)\n",
    "    df['is_transferred_day'] = df['date'].isin(transsferred_days).astype(int)\n",
    "    return df\n",
    "\n",
    "# create features for train and test sets\n",
    "train_df = add_holiday_features(train_df)\n",
    "print_null_rows(train_df, 'train_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95d036fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 41), No null rows in train_df_final.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.merge(oil_df, on='date', how='left')\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fad06ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 45), No null rows in train_df_final.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.merge(stores_df, on='store_nbr', how='left')\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e799dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 46), Null rows in train_df_final:\n",
      "              id       date  store_nbr      family  sales  onpromotion  year  \\\n",
      "0              0 2013-01-01          1  AUTOMOTIVE    0.0            0  2013   \n",
      "364       648648 2014-01-01          1  AUTOMOTIVE    0.0            0  2014   \n",
      "728      1297296 2015-01-01          1  AUTOMOTIVE    0.0            0  2015   \n",
      "915      1630530 2015-07-07          1  AUTOMOTIVE    0.0            0  2015   \n",
      "1092     1945944 2016-01-01          1  AUTOMOTIVE    0.0            0  2016   \n",
      "...          ...        ...        ...         ...    ...          ...   ...   \n",
      "2999932  1298945 2015-01-01         54     SEAFOOD    0.0            0  2015   \n",
      "3000296  1947593 2016-01-01         54     SEAFOOD    0.0            0  2016   \n",
      "3000298  1951157 2016-01-03         54     SEAFOOD    2.0            0  2016   \n",
      "3000299  1952939 2016-01-04         54     SEAFOOD    3.0            0  2016   \n",
      "3000661  2598023 2017-01-01         54     SEAFOOD    0.0            0  2017   \n",
      "\n",
      "         month  day  dayofweek  ...  is_working_day  is_event  is_bridge_day  \\\n",
      "0            1    1          1  ...               0         0              0   \n",
      "364          1    1          2  ...               0         0              0   \n",
      "728          1    1          3  ...               0         0              0   \n",
      "915          7    7          1  ...               0         0              0   \n",
      "1092         1    1          4  ...               0         0              0   \n",
      "...        ...  ...        ...  ...             ...       ...            ...   \n",
      "2999932      1    1          3  ...               0         0              0   \n",
      "3000296      1    1          4  ...               0         0              0   \n",
      "3000298      1    3          6  ...               0         0              0   \n",
      "3000299      1    4          0  ...               0         0              0   \n",
      "3000661      1    1          6  ...               0         0              0   \n",
      "\n",
      "         is_transferred_day  dcoilwtico       city      state  type  cluster  \\\n",
      "0                         0   93.140000      Quito  Pichincha     D       13   \n",
      "364                       0   96.809553      Quito  Pichincha     D       13   \n",
      "728                       0   52.981201      Quito  Pichincha     D       13   \n",
      "915                       0   52.330000      Quito  Pichincha     D       13   \n",
      "1092                      0   37.633604      Quito  Pichincha     D       13   \n",
      "...                     ...         ...        ...        ...   ...      ...   \n",
      "2999932                   0   52.981201  El Carmen     Manabi     C        3   \n",
      "3000296                   0   37.633604  El Carmen     Manabi     C        3   \n",
      "3000298                   0   37.290982  El Carmen     Manabi     C        3   \n",
      "3000299                   0   36.810000  El Carmen     Manabi     C        3   \n",
      "3000661                   1   52.655576  El Carmen     Manabi     C        3   \n",
      "\n",
      "         transactions  \n",
      "0                 NaN  \n",
      "364               NaN  \n",
      "728               NaN  \n",
      "915               NaN  \n",
      "1092              NaN  \n",
      "...               ...  \n",
      "2999932           NaN  \n",
      "3000296           NaN  \n",
      "3000298           NaN  \n",
      "3000299           NaN  \n",
      "3000661           NaN  \n",
      "\n",
      "[245784 rows x 46 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.merge(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93326774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 46), No null rows in train_df_final.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill missing values in transactions with 0\n",
    "train_df['transactions'] = train_df['transactions'].fillna(0)\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f11d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../data/interim/Darts_final_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
