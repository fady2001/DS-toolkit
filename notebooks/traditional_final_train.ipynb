{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61fdbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac17f0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000888, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/raw/train.csv', parse_dates=['date'])\n",
    "holiday_df = pd.read_csv('../data/raw/holidays_events.csv', parse_dates=['date'])\n",
    "oil_df = pd.read_csv('../data/raw/oil.csv', parse_dates=['date'])\n",
    "stores_df = pd.read_csv('../data/raw/stores.csv')\n",
    "transactions_df = pd.read_csv('../data/raw/transactions.csv', parse_dates=['date'])\n",
    "test_df = pd.read_csv('../data/raw/test.csv', parse_dates=['date'])\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5de525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 6), No null rows in train_df.\n",
      "\n",
      "shape:(350, 6), No null rows in holiday_df.\n",
      "\n",
      "shape:(1218, 2), Null rows in oil_df:\n",
      "           date  dcoilwtico\n",
      "0    2013-01-01         NaN\n",
      "14   2013-01-21         NaN\n",
      "34   2013-02-18         NaN\n",
      "63   2013-03-29         NaN\n",
      "104  2013-05-27         NaN\n",
      "132  2013-07-04         NaN\n",
      "174  2013-09-02         NaN\n",
      "237  2013-11-28         NaN\n",
      "256  2013-12-25         NaN\n",
      "261  2014-01-01         NaN\n",
      "274  2014-01-20         NaN\n",
      "294  2014-02-17         NaN\n",
      "338  2014-04-18         NaN\n",
      "364  2014-05-26         NaN\n",
      "393  2014-07-04         NaN\n",
      "434  2014-09-01         NaN\n",
      "497  2014-11-27         NaN\n",
      "517  2014-12-25         NaN\n",
      "522  2015-01-01         NaN\n",
      "534  2015-01-19         NaN\n",
      "554  2015-02-16         NaN\n",
      "588  2015-04-03         NaN\n",
      "624  2015-05-25         NaN\n",
      "653  2015-07-03         NaN\n",
      "699  2015-09-07         NaN\n",
      "757  2015-11-26         NaN\n",
      "778  2015-12-25         NaN\n",
      "783  2016-01-01         NaN\n",
      "794  2016-01-18         NaN\n",
      "814  2016-02-15         NaN\n",
      "843  2016-03-25         NaN\n",
      "889  2016-05-30         NaN\n",
      "914  2016-07-04         NaN\n",
      "959  2016-09-05         NaN\n",
      "1017 2016-11-24         NaN\n",
      "1039 2016-12-26         NaN\n",
      "1044 2017-01-02         NaN\n",
      "1054 2017-01-16         NaN\n",
      "1079 2017-02-20         NaN\n",
      "1118 2017-04-14         NaN\n",
      "1149 2017-05-29         NaN\n",
      "1174 2017-07-03         NaN\n",
      "1175 2017-07-04         NaN\n",
      "\n",
      "shape:(54, 5), No null rows in stores_df.\n",
      "\n",
      "shape:(83488, 3), No null rows in transactions_df.\n",
      "\n",
      "shape:(28512, 5), No null rows in test_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print null rows of any dataframe\n",
    "def print_null_rows(df, name):\n",
    "    null_rows = df[df.isnull().any(axis=1)]\n",
    "    if not null_rows.empty:\n",
    "        print(f\"shape:{df.shape}, Null rows in {name}:\\n{null_rows}\\n\")\n",
    "    else:\n",
    "        print(f\"shape:{df.shape}, No null rows in {name}.\\n\")\n",
    "    \n",
    "print_null_rows(train_df, 'train_df')\n",
    "print_null_rows(holiday_df, 'holiday_df')\n",
    "print_null_rows(oil_df, 'oil_df')\n",
    "print_null_rows(stores_df, 'stores_df')\n",
    "print_null_rows(transactions_df, 'transactions_df')\n",
    "print_null_rows(test_df, 'test_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a29a7",
   "metadata": {},
   "source": [
    "## 1. Traditional Final Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e77c8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate missing values in oil prices\n",
    "all_dates = pd.date_range(start=oil_df['date'].min(), end=oil_df['date'].max())\n",
    "oil_df = oil_df.set_index('date').reindex(all_dates).rename_axis('date').reset_index()\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].interpolate(method='polynomial', order=2)\n",
    "# fill backward and forward fill for oil prices\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d599f7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 6), No null rows in train_df.\n",
      "\n",
      "shape:(350, 6), No null rows in holiday_df.\n",
      "\n",
      "shape:(1704, 2), No null rows in oil_df.\n",
      "\n",
      "shape:(54, 5), No null rows in stores_df.\n",
      "\n",
      "shape:(83488, 3), No null rows in transactions_df.\n",
      "\n",
      "shape:(28512, 5), No null rows in test_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_null_rows(train_df, 'train_df')\n",
    "print_null_rows(holiday_df, 'holiday_df')\n",
    "print_null_rows(oil_df, 'oil_df')\n",
    "print_null_rows(stores_df, 'stores_df')\n",
    "print_null_rows(transactions_df, 'transactions_df')\n",
    "print_null_rows(test_df, 'test_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6203a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create faetures for train and test sets\n",
    "def create_features(df):\n",
    "    def days_since_payday(date):\n",
    "        day = date.day\n",
    "        if day <= 15:\n",
    "            # Days since last month's end\n",
    "            last_month_end = date.replace(day=1) - timedelta(days=1)\n",
    "            return (date - last_month_end).days\n",
    "        else:\n",
    "            # Days since 15th of current month\n",
    "            current_month_15th = date.replace(day=15)\n",
    "            return (date - current_month_15th).days\n",
    "        \n",
    "    def days_until_payday(date):\n",
    "        day = date.day\n",
    "        if day < 15:\n",
    "            # Days until 15th\n",
    "            return 15 - day\n",
    "        else:\n",
    "            # Days until month end\n",
    "            next_month = date.replace(day=28) + timedelta(days=4)\n",
    "            month_end = next_month - timedelta(days=next_month.day)\n",
    "            return (month_end - date).days\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
    "    df['is_payday'] = ((df['day'] == 15) | df['date'].dt.is_month_end).astype(int)\n",
    "    df['days_since_payday'] = df['date'].apply(days_since_payday)\n",
    "    df['days_until_payday'] = df['date'].apply(days_until_payday)\n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, target_col='sales', lags=[1, 7, 14, 30]):\n",
    "    \"\"\"Create lag features for time series\"\"\"\n",
    "    df_sorted = df.sort_values(['store_nbr', 'family', 'date'])\n",
    "    \n",
    "    for lag in lags:\n",
    "        df_sorted[f'{target_col}_lag_{lag}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].shift(lag)\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "def create_rolling_features(df, target_col='sales', windows=[7, 14, 30]):\n",
    "    \"\"\"Create rolling window statistics\"\"\"\n",
    "    df_sorted = df.sort_values(['store_nbr', 'family', 'date'])\n",
    "    \n",
    "    for window in windows:\n",
    "        # Rolling mean\n",
    "        df_sorted[f'{target_col}_rolling_mean_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Rolling std\n",
    "        df_sorted[f'{target_col}_rolling_std_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "        \n",
    "        # Rolling max\n",
    "        df_sorted[f'{target_col}_rolling_max_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "        )\n",
    "        \n",
    "        # Rolling min\n",
    "        df_sorted[f'{target_col}_rolling_min_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "        )\n",
    "    \n",
    "    return df_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af9dfb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 20), No null rows in train_df_temporal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = create_features(train_df)\n",
    "print_null_rows(train_df, 'train_df_temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3ed0620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          0\n",
       "date                        0\n",
       "store_nbr                   0\n",
       "family                      0\n",
       "sales                       0\n",
       "onpromotion                 0\n",
       "year                        0\n",
       "month                       0\n",
       "day                         0\n",
       "dayofweek                   0\n",
       "weekofyear                  0\n",
       "day_of_year                 0\n",
       "is_weekend                  0\n",
       "is_month_start              0\n",
       "is_month_end                0\n",
       "is_quarter_start            0\n",
       "is_quarter_end              0\n",
       "is_payday                   0\n",
       "days_since_payday           0\n",
       "days_until_payday           0\n",
       "sales_rolling_mean_7        0\n",
       "sales_rolling_std_7      1782\n",
       "sales_rolling_max_7         0\n",
       "sales_rolling_min_7         0\n",
       "sales_rolling_mean_14       0\n",
       "sales_rolling_std_14     1782\n",
       "sales_rolling_max_14        0\n",
       "sales_rolling_min_14        0\n",
       "sales_rolling_mean_30       0\n",
       "sales_rolling_std_30     1782\n",
       "sales_rolling_max_30        0\n",
       "sales_rolling_min_30        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = create_rolling_features(train_df)\n",
    "# print number of nulls in train_df_rolling\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "289d9e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "date                         0\n",
       "store_nbr                    0\n",
       "family                       0\n",
       "sales                        0\n",
       "onpromotion                  0\n",
       "year                         0\n",
       "month                        0\n",
       "day                          0\n",
       "dayofweek                    0\n",
       "weekofyear                   0\n",
       "day_of_year                  0\n",
       "is_weekend                   0\n",
       "is_month_start               0\n",
       "is_month_end                 0\n",
       "is_quarter_start             0\n",
       "is_quarter_end               0\n",
       "is_payday                    0\n",
       "days_since_payday            0\n",
       "days_until_payday            0\n",
       "sales_rolling_mean_7         0\n",
       "sales_rolling_std_7       1782\n",
       "sales_rolling_max_7          0\n",
       "sales_rolling_min_7          0\n",
       "sales_rolling_mean_14        0\n",
       "sales_rolling_std_14      1782\n",
       "sales_rolling_max_14         0\n",
       "sales_rolling_min_14         0\n",
       "sales_rolling_mean_30        0\n",
       "sales_rolling_std_30      1782\n",
       "sales_rolling_max_30         0\n",
       "sales_rolling_min_30         0\n",
       "sales_lag_1               1782\n",
       "sales_lag_7              12474\n",
       "sales_lag_14             24948\n",
       "sales_lag_30             53460\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = create_lag_features(train_df)\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0b4f595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(2947428, 36), No null rows in train_df_final.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7612d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "national_holidays = holiday_df[holiday_df['locale'] == 'National']['date'].unique()\n",
    "regional_holidays = holiday_df[holiday_df['locale'] == 'Regional']['date'].unique()\n",
    "local_holidays = holiday_df[holiday_df['locale'] == 'Local']['date'].unique()\n",
    "additional_holidays = holiday_df[holiday_df['type'] == 'Additional']['date'].unique()\n",
    "working_days = holiday_df[holiday_df['type'] == 'Work Day']['date'].unique()\n",
    "events = holiday_df[holiday_df['type'] == 'Event']['date'].unique()\n",
    "bridge_days = holiday_df[holiday_df['type'] == 'Bridge']['date'].unique()\n",
    "transsferred_days = holiday_df[holiday_df['transferred'] == True]['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "991f35ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(2947428, 44), No null rows in train_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add holiday features to train\n",
    "def add_holiday_features(df):\n",
    "    df['is_national_holiday'] = df['date'].isin(national_holidays).astype(int)\n",
    "    df['is_regional_holiday'] = df['date'].isin(regional_holidays).astype(int)\n",
    "    df['is_local_holiday'] = df['date'].isin(local_holidays).astype(int)\n",
    "    df['is_additional_holiday'] = df['date'].isin(additional_holidays).astype(int)\n",
    "    df['is_working_day'] = df['date'].isin(working_days).astype(int)\n",
    "    df['is_event'] = df['date'].isin(events).astype(int)\n",
    "    df['is_bridge_day'] = df['date'].isin(bridge_days).astype(int)\n",
    "    df['is_transferred_day'] = df['date'].isin(transsferred_days).astype(int)\n",
    "    return df\n",
    "\n",
    "# create features for train and test sets\n",
    "train_df = add_holiday_features(train_df)\n",
    "print_null_rows(train_df, 'train_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d036fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(oil_df, on='date', how='left')\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad06ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(stores_df, on='store_nbr', how='left')\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e799dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(2947428, 50), Null rows in train_df_final:\n",
      "              id       date  store_nbr      family  sales  onpromotion  year  \\\n",
      "334       648648 2014-01-01          1  AUTOMOTIVE    0.0            0  2014   \n",
      "698      1297296 2015-01-01          1  AUTOMOTIVE    0.0            0  2015   \n",
      "885      1630530 2015-07-07          1  AUTOMOTIVE    0.0            0  2015   \n",
      "1062     1945944 2016-01-01          1  AUTOMOTIVE    0.0            0  2016   \n",
      "1063     1947726 2016-01-02          1  AUTOMOTIVE    7.0            0  2016   \n",
      "...          ...        ...        ...         ...    ...          ...   ...   \n",
      "2946472  1298945 2015-01-01         54     SEAFOOD    0.0            0  2015   \n",
      "2946836  1947593 2016-01-01         54     SEAFOOD    0.0            0  2016   \n",
      "2946838  1951157 2016-01-03         54     SEAFOOD    2.0            0  2016   \n",
      "2946839  1952939 2016-01-04         54     SEAFOOD    3.0            0  2016   \n",
      "2947201  2598023 2017-01-01         54     SEAFOOD    0.0            0  2017   \n",
      "\n",
      "         month  day  dayofweek  ...  is_working_day  is_event  is_bridge_day  \\\n",
      "334          1    1          2  ...               0         0              0   \n",
      "698          1    1          3  ...               0         0              0   \n",
      "885          7    7          1  ...               0         0              0   \n",
      "1062         1    1          4  ...               0         0              0   \n",
      "1063         1    2          5  ...               0         0              0   \n",
      "...        ...  ...        ...  ...             ...       ...            ...   \n",
      "2946472      1    1          3  ...               0         0              0   \n",
      "2946836      1    1          4  ...               0         0              0   \n",
      "2946838      1    3          6  ...               0         0              0   \n",
      "2946839      1    4          0  ...               0         0              0   \n",
      "2947201      1    1          6  ...               0         0              0   \n",
      "\n",
      "         is_transferred_day  dcoilwtico       city      state  type  cluster  \\\n",
      "334                       0   96.809553      Quito  Pichincha     D       13   \n",
      "698                       0   52.981201      Quito  Pichincha     D       13   \n",
      "885                       0   52.330000      Quito  Pichincha     D       13   \n",
      "1062                      0   37.633604      Quito  Pichincha     D       13   \n",
      "1063                      0   37.626391      Quito  Pichincha     D       13   \n",
      "...                     ...         ...        ...        ...   ...      ...   \n",
      "2946472                   0   52.981201  El Carmen     Manabi     C        3   \n",
      "2946836                   0   37.633604  El Carmen     Manabi     C        3   \n",
      "2946838                   0   37.290982  El Carmen     Manabi     C        3   \n",
      "2946839                   0   36.810000  El Carmen     Manabi     C        3   \n",
      "2947201                   1   52.655576  El Carmen     Manabi     C        3   \n",
      "\n",
      "         transactions  \n",
      "334               NaN  \n",
      "698               NaN  \n",
      "885               NaN  \n",
      "1062              NaN  \n",
      "1063              NaN  \n",
      "...               ...  \n",
      "2946472           NaN  \n",
      "2946836           NaN  \n",
      "2946838           NaN  \n",
      "2946839           NaN  \n",
      "2947201           NaN  \n",
      "\n",
      "[236379 rows x 50 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.merge(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93326774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(2947428, 50), No null rows in train_df_final.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill missing values in transactions with 0\n",
    "train_df['transactions'] = train_df['transactions'].fillna(0)\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../data/interim/traditional_final_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
