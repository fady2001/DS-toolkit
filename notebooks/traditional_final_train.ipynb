{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a61fdbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac17f0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000888, 6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/raw/train.csv', parse_dates=['date'])\n",
    "holiday_df = pd.read_csv('../data/raw/holidays_events.csv', parse_dates=['date'])\n",
    "oil_df = pd.read_csv('../data/raw/oil.csv', parse_dates=['date'])\n",
    "stores_df = pd.read_csv('../data/raw/stores.csv')\n",
    "transactions_df = pd.read_csv('../data/raw/transactions.csv', parse_dates=['date'])\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb5de525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 6), No null rows in train_df.\n",
      "\n",
      "shape:(350, 6), No null rows in holiday_df.\n",
      "\n",
      "shape:(1218, 2), Null rows in oil_df:\n",
      "           date  dcoilwtico\n",
      "0    2013-01-01         NaN\n",
      "14   2013-01-21         NaN\n",
      "34   2013-02-18         NaN\n",
      "63   2013-03-29         NaN\n",
      "104  2013-05-27         NaN\n",
      "132  2013-07-04         NaN\n",
      "174  2013-09-02         NaN\n",
      "237  2013-11-28         NaN\n",
      "256  2013-12-25         NaN\n",
      "261  2014-01-01         NaN\n",
      "274  2014-01-20         NaN\n",
      "294  2014-02-17         NaN\n",
      "338  2014-04-18         NaN\n",
      "364  2014-05-26         NaN\n",
      "393  2014-07-04         NaN\n",
      "434  2014-09-01         NaN\n",
      "497  2014-11-27         NaN\n",
      "517  2014-12-25         NaN\n",
      "522  2015-01-01         NaN\n",
      "534  2015-01-19         NaN\n",
      "554  2015-02-16         NaN\n",
      "588  2015-04-03         NaN\n",
      "624  2015-05-25         NaN\n",
      "653  2015-07-03         NaN\n",
      "699  2015-09-07         NaN\n",
      "757  2015-11-26         NaN\n",
      "778  2015-12-25         NaN\n",
      "783  2016-01-01         NaN\n",
      "794  2016-01-18         NaN\n",
      "814  2016-02-15         NaN\n",
      "843  2016-03-25         NaN\n",
      "889  2016-05-30         NaN\n",
      "914  2016-07-04         NaN\n",
      "959  2016-09-05         NaN\n",
      "1017 2016-11-24         NaN\n",
      "1039 2016-12-26         NaN\n",
      "1044 2017-01-02         NaN\n",
      "1054 2017-01-16         NaN\n",
      "1079 2017-02-20         NaN\n",
      "1118 2017-04-14         NaN\n",
      "1149 2017-05-29         NaN\n",
      "1174 2017-07-03         NaN\n",
      "1175 2017-07-04         NaN\n",
      "\n",
      "shape:(54, 5), No null rows in stores_df.\n",
      "\n",
      "shape:(83488, 3), No null rows in transactions_df.\n",
      "\n",
      "shape:(28512, 19), No null rows in test_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print null rows of any dataframe\n",
    "def print_null_rows(df, name):\n",
    "    null_rows = df[df.isnull().any(axis=1)]\n",
    "    if not null_rows.empty:\n",
    "        print(f\"shape:{df.shape}, Null rows in {name}:\\n{null_rows}\\n\")\n",
    "    else:\n",
    "        print(f\"shape:{df.shape}, No null rows in {name}.\\n\")\n",
    "    \n",
    "print_null_rows(train_df, 'train_df')\n",
    "print_null_rows(holiday_df, 'holiday_df')\n",
    "print_null_rows(oil_df, 'oil_df')\n",
    "print_null_rows(stores_df, 'stores_df')\n",
    "print_null_rows(transactions_df, 'transactions_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a29a7",
   "metadata": {},
   "source": [
    "## 1. Traditional Final Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e77c8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate missing values in oil prices\n",
    "all_dates = pd.date_range(start=oil_df['date'].min(), end=oil_df['date'].max())\n",
    "oil_df = oil_df.set_index('date').reindex(all_dates).rename_axis('date').reset_index()\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].interpolate(method='polynomial', order=2)\n",
    "# fill backward and forward fill for oil prices\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d599f7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 6), No null rows in train_df.\n",
      "\n",
      "shape:(350, 6), No null rows in holiday_df.\n",
      "\n",
      "shape:(1704, 2), No null rows in oil_df.\n",
      "\n",
      "shape:(54, 5), No null rows in stores_df.\n",
      "\n",
      "shape:(83488, 3), No null rows in transactions_df.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_null_rows(train_df, 'train_df')\n",
    "print_null_rows(holiday_df, 'holiday_df')\n",
    "print_null_rows(oil_df, 'oil_df')\n",
    "print_null_rows(stores_df, 'stores_df')\n",
    "print_null_rows(transactions_df, 'transactions_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6203a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create faetures for train and test sets\n",
    "def create_features(df):\n",
    "    def days_since_payday(date):\n",
    "        day = date.day\n",
    "        if day <= 15:\n",
    "            # Days since last month's end\n",
    "            last_month_end = date.replace(day=1) - timedelta(days=1)\n",
    "            return (date - last_month_end).days\n",
    "        else:\n",
    "            # Days since 15th of current month\n",
    "            current_month_15th = date.replace(day=15)\n",
    "            return (date - current_month_15th).days\n",
    "        \n",
    "    def days_until_payday(date):\n",
    "        day = date.day\n",
    "        if day < 15:\n",
    "            # Days until 15th\n",
    "            return 15 - day\n",
    "        else:\n",
    "            # Days until month end\n",
    "            next_month = date.replace(day=28) + timedelta(days=4)\n",
    "            month_end = next_month - timedelta(days=next_month.day)\n",
    "            return (month_end - date).days\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
    "    df['is_payday'] = ((df['day'] == 15) | df['date'].dt.is_month_end).astype(int)\n",
    "    df['days_since_payday'] = df['date'].apply(days_since_payday)\n",
    "    df['days_until_payday'] = df['date'].apply(days_until_payday)\n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, target_col='sales', lags=[1, 7, 14, 30]):\n",
    "    \"\"\"Create lag features for time series\"\"\"\n",
    "    df_sorted = df.sort_values(['store_nbr', 'family', 'date'])\n",
    "    \n",
    "    for lag in lags:\n",
    "        df_sorted[f'{target_col}_lag_{lag}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].shift(lag)\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "def create_rolling_features(df, target_col='sales', windows=[7, 14, 30]):\n",
    "    \"\"\"Create rolling window statistics\"\"\"\n",
    "    df_sorted = df.sort_values(['store_nbr', 'family', 'date'])\n",
    "    \n",
    "    for window in windows:\n",
    "        # Rolling mean\n",
    "        df_sorted[f'{target_col}_rolling_mean_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Rolling std\n",
    "        df_sorted[f'{target_col}_rolling_std_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "        \n",
    "        # Rolling max\n",
    "        df_sorted[f'{target_col}_rolling_max_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "        )\n",
    "        \n",
    "        # Rolling min\n",
    "        df_sorted[f'{target_col}_rolling_min_{window}'] = df_sorted.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "        )\n",
    "    \n",
    "    return df_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af9dfb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(3000888, 20), No null rows in train_df_temporal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = create_features(train_df)\n",
    "print_null_rows(train_df, 'train_df_temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3ed0620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          0\n",
       "date                        0\n",
       "store_nbr                   0\n",
       "family                      0\n",
       "sales                       0\n",
       "onpromotion                 0\n",
       "year                        0\n",
       "month                       0\n",
       "day                         0\n",
       "dayofweek                   0\n",
       "weekofyear                  0\n",
       "day_of_year                 0\n",
       "is_weekend                  0\n",
       "is_month_start              0\n",
       "is_month_end                0\n",
       "is_quarter_start            0\n",
       "is_quarter_end              0\n",
       "is_payday                   0\n",
       "days_since_payday           0\n",
       "days_until_payday           0\n",
       "sales_rolling_mean_7        0\n",
       "sales_rolling_std_7      1782\n",
       "sales_rolling_max_7         0\n",
       "sales_rolling_min_7         0\n",
       "sales_rolling_mean_14       0\n",
       "sales_rolling_std_14     1782\n",
       "sales_rolling_max_14        0\n",
       "sales_rolling_min_14        0\n",
       "sales_rolling_mean_30       0\n",
       "sales_rolling_std_30     1782\n",
       "sales_rolling_max_30        0\n",
       "sales_rolling_min_30        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = create_rolling_features(train_df)\n",
    "# print number of nulls in train_df_rolling\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "289d9e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "date                         0\n",
       "store_nbr                    0\n",
       "family                       0\n",
       "sales                        0\n",
       "onpromotion                  0\n",
       "year                         0\n",
       "month                        0\n",
       "day                          0\n",
       "dayofweek                    0\n",
       "weekofyear                   0\n",
       "day_of_year                  0\n",
       "is_weekend                   0\n",
       "is_month_start               0\n",
       "is_month_end                 0\n",
       "is_quarter_start             0\n",
       "is_quarter_end               0\n",
       "is_payday                    0\n",
       "days_since_payday            0\n",
       "days_until_payday            0\n",
       "sales_rolling_mean_7         0\n",
       "sales_rolling_std_7       1782\n",
       "sales_rolling_max_7          0\n",
       "sales_rolling_min_7          0\n",
       "sales_rolling_mean_14        0\n",
       "sales_rolling_std_14      1782\n",
       "sales_rolling_max_14         0\n",
       "sales_rolling_min_14         0\n",
       "sales_rolling_mean_30        0\n",
       "sales_rolling_std_30      1782\n",
       "sales_rolling_max_30         0\n",
       "sales_rolling_min_30         0\n",
       "sales_lag_1               1782\n",
       "sales_lag_7              12474\n",
       "sales_lag_14             24948\n",
       "sales_lag_30             53460\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = create_lag_features(train_df)\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0b4f595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(2947428, 36), No null rows in train_df_final.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7612d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "national_holidays = holiday_df[holiday_df['locale'] == 'National']['date'].unique()\n",
    "regional_holidays = holiday_df[holiday_df['locale'] == 'Regional']['date'].unique()\n",
    "local_holidays = holiday_df[holiday_df['locale'] == 'Local']['date'].unique()\n",
    "additional_holidays = holiday_df[holiday_df['type'] == 'Additional']['date'].unique()\n",
    "working_days = holiday_df[holiday_df['type'] == 'Work Day']['date'].unique()\n",
    "events = holiday_df[holiday_df['type'] == 'Event']['date'].unique()\n",
    "bridge_days = holiday_df[holiday_df['type'] == 'Bridge']['date'].unique()\n",
    "transsferred_days = holiday_df[holiday_df['transferred'] == True]['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "991f35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add holiday features to train\n",
    "def add_holiday_features(df):\n",
    "    df['is_national_holiday'] = df['date'].isin(national_holidays).astype(int)\n",
    "    df['is_regional_holiday'] = df['date'].isin(regional_holidays).astype(int)\n",
    "    df['is_local_holiday'] = df['date'].isin(local_holidays).astype(int)\n",
    "    df['is_additional_holiday'] = df['date'].isin(additional_holidays).astype(int)\n",
    "    df['is_working_day'] = df['date'].isin(working_days).astype(int)\n",
    "    df['is_event'] = df['date'].isin(events).astype(int)\n",
    "    df['is_bridge_day'] = df['date'].isin(bridge_days).astype(int)\n",
    "    df['is_transferred_day'] = df['date'].isin(transsferred_days).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features for train and test sets\n",
    "train_df = add_holiday_features(train_df)\n",
    "print_null_rows(train_df, 'train_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d036fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(oil_df, on='date', how='left')\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad06ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(stores_df, on='store_nbr', how='left')\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e799dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(2947428, 50), Null rows in train_df_final:\n",
      "              id       date  store_nbr      family  sales  onpromotion  year  \\\n",
      "334       648648 2014-01-01          1  AUTOMOTIVE    0.0            0  2014   \n",
      "698      1297296 2015-01-01          1  AUTOMOTIVE    0.0            0  2015   \n",
      "885      1630530 2015-07-07          1  AUTOMOTIVE    0.0            0  2015   \n",
      "1062     1945944 2016-01-01          1  AUTOMOTIVE    0.0            0  2016   \n",
      "1063     1947726 2016-01-02          1  AUTOMOTIVE    7.0            0  2016   \n",
      "...          ...        ...        ...         ...    ...          ...   ...   \n",
      "2946472  1298945 2015-01-01         54     SEAFOOD    0.0            0  2015   \n",
      "2946836  1947593 2016-01-01         54     SEAFOOD    0.0            0  2016   \n",
      "2946838  1951157 2016-01-03         54     SEAFOOD    2.0            0  2016   \n",
      "2946839  1952939 2016-01-04         54     SEAFOOD    3.0            0  2016   \n",
      "2947201  2598023 2017-01-01         54     SEAFOOD    0.0            0  2017   \n",
      "\n",
      "         month  day  dayofweek  ...  is_working_day  is_event  is_bridge_day  \\\n",
      "334          1    1          2  ...               0         0              0   \n",
      "698          1    1          3  ...               0         0              0   \n",
      "885          7    7          1  ...               0         0              0   \n",
      "1062         1    1          4  ...               0         0              0   \n",
      "1063         1    2          5  ...               0         0              0   \n",
      "...        ...  ...        ...  ...             ...       ...            ...   \n",
      "2946472      1    1          3  ...               0         0              0   \n",
      "2946836      1    1          4  ...               0         0              0   \n",
      "2946838      1    3          6  ...               0         0              0   \n",
      "2946839      1    4          0  ...               0         0              0   \n",
      "2947201      1    1          6  ...               0         0              0   \n",
      "\n",
      "         is_transferred_day  dcoilwtico       city      state  type  cluster  \\\n",
      "334                       0   96.809553      Quito  Pichincha     D       13   \n",
      "698                       0   52.981201      Quito  Pichincha     D       13   \n",
      "885                       0   52.330000      Quito  Pichincha     D       13   \n",
      "1062                      0   37.633604      Quito  Pichincha     D       13   \n",
      "1063                      0   37.626391      Quito  Pichincha     D       13   \n",
      "...                     ...         ...        ...        ...   ...      ...   \n",
      "2946472                   0   52.981201  El Carmen     Manabi     C        3   \n",
      "2946836                   0   37.633604  El Carmen     Manabi     C        3   \n",
      "2946838                   0   37.290982  El Carmen     Manabi     C        3   \n",
      "2946839                   0   36.810000  El Carmen     Manabi     C        3   \n",
      "2947201                   1   52.655576  El Carmen     Manabi     C        3   \n",
      "\n",
      "         transactions  \n",
      "334               NaN  \n",
      "698               NaN  \n",
      "885               NaN  \n",
      "1062              NaN  \n",
      "1063              NaN  \n",
      "...               ...  \n",
      "2946472           NaN  \n",
      "2946836           NaN  \n",
      "2946838           NaN  \n",
      "2946839           NaN  \n",
      "2947201           NaN  \n",
      "\n",
      "[236379 rows x 50 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.merge(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93326774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(2947428, 50), No null rows in train_df_final.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill missing values in transactions with 0\n",
    "train_df['transactions'] = train_df['transactions'].fillna(0)\n",
    "print_null_rows(train_df, 'train_df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../data/interim/traditional_final_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ce54b",
   "metadata": {},
   "source": [
    "## 2. Test Data Feature Engineering (Avoiding Data Leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3b643d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/raw/test.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b8b014e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28512, 5), No null rows in test_df.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "date           0\n",
       "store_nbr      0\n",
       "family         0\n",
       "onpromotion    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_null_rows(test_df, 'test_df')\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "594be805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28512, 19), No null rows in test_df_temporal.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "date                 0\n",
       "store_nbr            0\n",
       "family               0\n",
       "onpromotion          0\n",
       "year                 0\n",
       "month                0\n",
       "day                  0\n",
       "dayofweek            0\n",
       "weekofyear           0\n",
       "day_of_year          0\n",
       "is_weekend           0\n",
       "is_month_start       0\n",
       "is_month_end         0\n",
       "is_quarter_start     0\n",
       "is_quarter_end       0\n",
       "is_payday            0\n",
       "days_since_payday    0\n",
       "days_until_payday    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = create_features(test_df)\n",
    "print_null_rows(test_df, 'test_df_temporal')\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "934caf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last training date: 2017-08-15 00:00:00\n",
      "First test date: 2017-08-16 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# For test data, we need to be careful about lag and rolling features\n",
    "# We'll use the last available training data to create these features\n",
    "\n",
    "# Get the last date from training data\n",
    "last_train_date = train_df['date'].max()\n",
    "print(f\"Last training date: {last_train_date}\")\n",
    "\n",
    "# Get the first date from test data\n",
    "first_test_date = test_df['date'].min()\n",
    "print(f\"First test date: {first_test_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35d4182a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape with lag features: (28512, 24)\n"
     ]
    }
   ],
   "source": [
    "# Create a function to safely create lag features for test data\n",
    "def create_safe_lag_features_for_test(train_df, test_df, target_col='sales', lags=[1, 7, 14, 30]):\n",
    "    \"\"\"Create lag features for test data using only training data to avoid leakage\"\"\"\n",
    "    \n",
    "    # Combine train and test for consistent processing\n",
    "    # But we'll only use train data for lag calculations\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    combined_df = combined_df.sort_values(['store_nbr', 'family', 'date'])\n",
    "    \n",
    "    # Create lag features\n",
    "    for lag in lags:\n",
    "        combined_df[f'{target_col}_lag_{lag}'] = combined_df.groupby(['store_nbr', 'family'])[target_col].shift(lag)\n",
    "    \n",
    "    # Return only test data portion\n",
    "    test_with_lags = combined_df[combined_df['date'] >= first_test_date].copy()\n",
    "    return test_with_lags\n",
    "\n",
    "# Apply lag features to test data\n",
    "test_df_with_lags = create_safe_lag_features_for_test(train_df, test_df)\n",
    "print(f\"Test data shape with lag features: {test_df_with_lags.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f709d348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                       0\n",
       "date                     0\n",
       "store_nbr                0\n",
       "family                   0\n",
       "sales                28512\n",
       "onpromotion              0\n",
       "year                     0\n",
       "month                    0\n",
       "day                      0\n",
       "dayofweek                0\n",
       "weekofyear               0\n",
       "day_of_year              0\n",
       "is_weekend               0\n",
       "is_month_start           0\n",
       "is_month_end             0\n",
       "is_quarter_start         0\n",
       "is_quarter_end           0\n",
       "is_payday                0\n",
       "days_since_payday        0\n",
       "days_until_payday        0\n",
       "sales_lag_1          26730\n",
       "sales_lag_7          16038\n",
       "sales_lag_14          3564\n",
       "sales_lag_30             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_with_lags.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c62e21eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape with rolling features: (28512, 36)\n"
     ]
    }
   ],
   "source": [
    "# Create rolling features for test data using training data history\n",
    "def create_safe_rolling_features_for_test(train_df, test_df, target_col='sales', windows=[7, 14, 30]):\n",
    "    \"\"\"Create rolling features for test data using training data history\"\"\"\n",
    "    \n",
    "    # Combine train and test data\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    combined_df = combined_df.sort_values(['store_nbr', 'family', 'date'])\n",
    "    \n",
    "    for window in windows:\n",
    "        # Rolling mean\n",
    "        combined_df[f'{target_col}_rolling_mean_{window}'] = combined_df.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Rolling std\n",
    "        combined_df[f'{target_col}_rolling_std_{window}'] = combined_df.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "        \n",
    "        # Rolling max\n",
    "        combined_df[f'{target_col}_rolling_max_{window}'] = combined_df.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "        )\n",
    "        \n",
    "        # Rolling min\n",
    "        combined_df[f'{target_col}_rolling_min_{window}'] = combined_df.groupby(['store_nbr', 'family'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "        )\n",
    "    \n",
    "    # Return only test data portion\n",
    "    test_with_rolling = combined_df[combined_df['date'] >= first_test_date].copy()\n",
    "    return test_with_rolling\n",
    "\n",
    "# Apply rolling features to test data\n",
    "test_df_processed = create_safe_rolling_features_for_test(train_df, test_df_with_lags)\n",
    "print(f\"Test data shape with rolling features: {test_df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38117820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "date                         0\n",
       "store_nbr                    0\n",
       "family                       0\n",
       "sales                    28512\n",
       "onpromotion                  0\n",
       "year                         0\n",
       "month                        0\n",
       "day                          0\n",
       "dayofweek                    0\n",
       "weekofyear                   0\n",
       "day_of_year                  0\n",
       "is_weekend                   0\n",
       "is_month_start               0\n",
       "is_month_end                 0\n",
       "is_quarter_start             0\n",
       "is_quarter_end               0\n",
       "is_payday                    0\n",
       "days_since_payday            0\n",
       "days_until_payday            0\n",
       "sales_lag_1              26730\n",
       "sales_lag_7              16038\n",
       "sales_lag_14              3564\n",
       "sales_lag_30                 0\n",
       "sales_rolling_mean_7     17820\n",
       "sales_rolling_std_7      19602\n",
       "sales_rolling_max_7      17820\n",
       "sales_rolling_min_7      17820\n",
       "sales_rolling_mean_14     5346\n",
       "sales_rolling_std_14      7128\n",
       "sales_rolling_max_14      5346\n",
       "sales_rolling_min_14      5346\n",
       "sales_rolling_mean_30        0\n",
       "sales_rolling_std_30         0\n",
       "sales_rolling_max_30         0\n",
       "sales_rolling_min_30         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_processed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "914faa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28512, 44), Null rows in test_df_holidays:\n",
      "              id       date  store_nbr      family  sales  onpromotion  \\\n",
      "3000888  3000888 2017-08-16          1  AUTOMOTIVE    NaN            0   \n",
      "3000889  3002670 2017-08-17          1  AUTOMOTIVE    NaN            0   \n",
      "3000890  3004452 2017-08-18          1  AUTOMOTIVE    NaN            0   \n",
      "3000891  3006234 2017-08-19          1  AUTOMOTIVE    NaN            0   \n",
      "3000892  3008016 2017-08-20          1  AUTOMOTIVE    NaN            0   \n",
      "...          ...        ...        ...         ...    ...          ...   \n",
      "3029395  3022139 2017-08-27         54     SEAFOOD    NaN            0   \n",
      "3029396  3023921 2017-08-28         54     SEAFOOD    NaN            0   \n",
      "3029397  3025703 2017-08-29         54     SEAFOOD    NaN            0   \n",
      "3029398  3027485 2017-08-30         54     SEAFOOD    NaN            0   \n",
      "3029399  3029267 2017-08-31         54     SEAFOOD    NaN            0   \n",
      "\n",
      "           year  month   day  dayofweek  ...  sales_rolling_max_30  \\\n",
      "3000888  2017.0    8.0  16.0        2.0  ...                  10.0   \n",
      "3000889  2017.0    8.0  17.0        3.0  ...                  10.0   \n",
      "3000890  2017.0    8.0  18.0        4.0  ...                  10.0   \n",
      "3000891  2017.0    8.0  19.0        5.0  ...                  10.0   \n",
      "3000892  2017.0    8.0  20.0        6.0  ...                  10.0   \n",
      "...         ...    ...   ...        ...  ...                   ...   \n",
      "3029395  2017.0    8.0  27.0        6.0  ...                  12.0   \n",
      "3029396  2017.0    8.0  28.0        0.0  ...                  12.0   \n",
      "3029397  2017.0    8.0  29.0        1.0  ...                  12.0   \n",
      "3029398  2017.0    8.0  30.0        2.0  ...                  12.0   \n",
      "3029399  2017.0    8.0  31.0        3.0  ...                  12.0   \n",
      "\n",
      "         sales_rolling_min_30  is_national_holiday  is_regional_holiday  \\\n",
      "3000888                   0.0                    0                    0   \n",
      "3000889                   0.0                    0                    0   \n",
      "3000890                   0.0                    0                    0   \n",
      "3000891                   0.0                    0                    0   \n",
      "3000892                   0.0                    0                    0   \n",
      "...                       ...                  ...                  ...   \n",
      "3029395                   0.0                    0                    0   \n",
      "3029396                   0.0                    0                    0   \n",
      "3029397                   0.0                    0                    0   \n",
      "3029398                   0.0                    0                    0   \n",
      "3029399                   0.0                    0                    0   \n",
      "\n",
      "         is_local_holiday  is_additional_holiday  is_working_day  is_event  \\\n",
      "3000888                 0                      0               0         0   \n",
      "3000889                 0                      0               0         0   \n",
      "3000890                 0                      0               0         0   \n",
      "3000891                 0                      0               0         0   \n",
      "3000892                 0                      0               0         0   \n",
      "...                   ...                    ...             ...       ...   \n",
      "3029395                 0                      0               0         0   \n",
      "3029396                 0                      0               0         0   \n",
      "3029397                 0                      0               0         0   \n",
      "3029398                 0                      0               0         0   \n",
      "3029399                 0                      0               0         0   \n",
      "\n",
      "         is_bridge_day  is_transferred_day  \n",
      "3000888              0                   0  \n",
      "3000889              0                   0  \n",
      "3000890              0                   0  \n",
      "3000891              0                   0  \n",
      "3000892              0                   0  \n",
      "...                ...                 ...  \n",
      "3029395              0                   0  \n",
      "3029396              0                   0  \n",
      "3029397              0                   0  \n",
      "3029398              0                   0  \n",
      "3029399              0                   0  \n",
      "\n",
      "[28512 rows x 44 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add holiday features to test data\n",
    "test_df_processed = add_holiday_features(test_df_processed)\n",
    "print_null_rows(test_df_processed, 'test_df_holidays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28cba622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28512, 45), Null rows in test_df_oil:\n",
      "            id       date  store_nbr      family  sales  onpromotion    year  \\\n",
      "0      3000888 2017-08-16          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "1      3002670 2017-08-17          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "2      3004452 2017-08-18          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "3      3006234 2017-08-19          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "4      3008016 2017-08-20          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "...        ...        ...        ...         ...    ...          ...     ...   \n",
      "28507  3022139 2017-08-27         54     SEAFOOD    NaN            0  2017.0   \n",
      "28508  3023921 2017-08-28         54     SEAFOOD    NaN            0  2017.0   \n",
      "28509  3025703 2017-08-29         54     SEAFOOD    NaN            0  2017.0   \n",
      "28510  3027485 2017-08-30         54     SEAFOOD    NaN            0  2017.0   \n",
      "28511  3029267 2017-08-31         54     SEAFOOD    NaN            0  2017.0   \n",
      "\n",
      "       month   day  dayofweek  ...  sales_rolling_min_30  is_national_holiday  \\\n",
      "0        8.0  16.0        2.0  ...                   0.0                    0   \n",
      "1        8.0  17.0        3.0  ...                   0.0                    0   \n",
      "2        8.0  18.0        4.0  ...                   0.0                    0   \n",
      "3        8.0  19.0        5.0  ...                   0.0                    0   \n",
      "4        8.0  20.0        6.0  ...                   0.0                    0   \n",
      "...      ...   ...        ...  ...                   ...                  ...   \n",
      "28507    8.0  27.0        6.0  ...                   0.0                    0   \n",
      "28508    8.0  28.0        0.0  ...                   0.0                    0   \n",
      "28509    8.0  29.0        1.0  ...                   0.0                    0   \n",
      "28510    8.0  30.0        2.0  ...                   0.0                    0   \n",
      "28511    8.0  31.0        3.0  ...                   0.0                    0   \n",
      "\n",
      "       is_regional_holiday  is_local_holiday  is_additional_holiday  \\\n",
      "0                        0                 0                      0   \n",
      "1                        0                 0                      0   \n",
      "2                        0                 0                      0   \n",
      "3                        0                 0                      0   \n",
      "4                        0                 0                      0   \n",
      "...                    ...               ...                    ...   \n",
      "28507                    0                 0                      0   \n",
      "28508                    0                 0                      0   \n",
      "28509                    0                 0                      0   \n",
      "28510                    0                 0                      0   \n",
      "28511                    0                 0                      0   \n",
      "\n",
      "       is_working_day  is_event  is_bridge_day  is_transferred_day  dcoilwtico  \n",
      "0                   0         0              0                   0   46.800000  \n",
      "1                   0         0              0                   0   47.070000  \n",
      "2                   0         0              0                   0   48.590000  \n",
      "3                   0         0              0                   0   48.916095  \n",
      "4                   0         0              0                   0   47.927188  \n",
      "...               ...       ...            ...                 ...         ...  \n",
      "28507               0         0              0                   0   46.763127  \n",
      "28508               0         0              0                   0   46.400000  \n",
      "28509               0         0              0                   0   46.460000  \n",
      "28510               0         0              0                   0   45.960000  \n",
      "28511               0         0              0                   0   47.260000  \n",
      "\n",
      "[28512 rows x 45 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge oil prices with test data\n",
    "test_df_processed = test_df_processed.merge(oil_df, on='date', how='left')\n",
    "print_null_rows(test_df_processed, 'test_df_oil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "935fcef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28512, 49), Null rows in test_df_stores:\n",
      "            id       date  store_nbr      family  sales  onpromotion    year  \\\n",
      "0      3000888 2017-08-16          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "1      3002670 2017-08-17          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "2      3004452 2017-08-18          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "3      3006234 2017-08-19          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "4      3008016 2017-08-20          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "...        ...        ...        ...         ...    ...          ...     ...   \n",
      "28507  3022139 2017-08-27         54     SEAFOOD    NaN            0  2017.0   \n",
      "28508  3023921 2017-08-28         54     SEAFOOD    NaN            0  2017.0   \n",
      "28509  3025703 2017-08-29         54     SEAFOOD    NaN            0  2017.0   \n",
      "28510  3027485 2017-08-30         54     SEAFOOD    NaN            0  2017.0   \n",
      "28511  3029267 2017-08-31         54     SEAFOOD    NaN            0  2017.0   \n",
      "\n",
      "       month   day  dayofweek  ...  is_additional_holiday  is_working_day  \\\n",
      "0        8.0  16.0        2.0  ...                      0               0   \n",
      "1        8.0  17.0        3.0  ...                      0               0   \n",
      "2        8.0  18.0        4.0  ...                      0               0   \n",
      "3        8.0  19.0        5.0  ...                      0               0   \n",
      "4        8.0  20.0        6.0  ...                      0               0   \n",
      "...      ...   ...        ...  ...                    ...             ...   \n",
      "28507    8.0  27.0        6.0  ...                      0               0   \n",
      "28508    8.0  28.0        0.0  ...                      0               0   \n",
      "28509    8.0  29.0        1.0  ...                      0               0   \n",
      "28510    8.0  30.0        2.0  ...                      0               0   \n",
      "28511    8.0  31.0        3.0  ...                      0               0   \n",
      "\n",
      "       is_event  is_bridge_day  is_transferred_day  dcoilwtico       city  \\\n",
      "0             0              0                   0   46.800000      Quito   \n",
      "1             0              0                   0   47.070000      Quito   \n",
      "2             0              0                   0   48.590000      Quito   \n",
      "3             0              0                   0   48.916095      Quito   \n",
      "4             0              0                   0   47.927188      Quito   \n",
      "...         ...            ...                 ...         ...        ...   \n",
      "28507         0              0                   0   46.763127  El Carmen   \n",
      "28508         0              0                   0   46.400000  El Carmen   \n",
      "28509         0              0                   0   46.460000  El Carmen   \n",
      "28510         0              0                   0   45.960000  El Carmen   \n",
      "28511         0              0                   0   47.260000  El Carmen   \n",
      "\n",
      "           state  type  cluster  \n",
      "0      Pichincha     D       13  \n",
      "1      Pichincha     D       13  \n",
      "2      Pichincha     D       13  \n",
      "3      Pichincha     D       13  \n",
      "4      Pichincha     D       13  \n",
      "...          ...   ...      ...  \n",
      "28507     Manabi     C        3  \n",
      "28508     Manabi     C        3  \n",
      "28509     Manabi     C        3  \n",
      "28510     Manabi     C        3  \n",
      "28511     Manabi     C        3  \n",
      "\n",
      "[28512 rows x 49 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge store information\n",
    "test_df_processed = test_df_processed.merge(stores_df, on='store_nbr', how='left')\n",
    "print_null_rows(test_df_processed, 'test_df_stores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e11a0f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(28512, 50), Null rows in test_df_transactions:\n",
      "            id       date  store_nbr      family  sales  onpromotion    year  \\\n",
      "0      3000888 2017-08-16          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "1      3002670 2017-08-17          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "2      3004452 2017-08-18          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "3      3006234 2017-08-19          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "4      3008016 2017-08-20          1  AUTOMOTIVE    NaN            0  2017.0   \n",
      "...        ...        ...        ...         ...    ...          ...     ...   \n",
      "28507  3022139 2017-08-27         54     SEAFOOD    NaN            0  2017.0   \n",
      "28508  3023921 2017-08-28         54     SEAFOOD    NaN            0  2017.0   \n",
      "28509  3025703 2017-08-29         54     SEAFOOD    NaN            0  2017.0   \n",
      "28510  3027485 2017-08-30         54     SEAFOOD    NaN            0  2017.0   \n",
      "28511  3029267 2017-08-31         54     SEAFOOD    NaN            0  2017.0   \n",
      "\n",
      "       month   day  dayofweek  ...  is_working_day  is_event  is_bridge_day  \\\n",
      "0        8.0  16.0        2.0  ...               0         0              0   \n",
      "1        8.0  17.0        3.0  ...               0         0              0   \n",
      "2        8.0  18.0        4.0  ...               0         0              0   \n",
      "3        8.0  19.0        5.0  ...               0         0              0   \n",
      "4        8.0  20.0        6.0  ...               0         0              0   \n",
      "...      ...   ...        ...  ...             ...       ...            ...   \n",
      "28507    8.0  27.0        6.0  ...               0         0              0   \n",
      "28508    8.0  28.0        0.0  ...               0         0              0   \n",
      "28509    8.0  29.0        1.0  ...               0         0              0   \n",
      "28510    8.0  30.0        2.0  ...               0         0              0   \n",
      "28511    8.0  31.0        3.0  ...               0         0              0   \n",
      "\n",
      "       is_transferred_day  dcoilwtico       city      state  type  cluster  \\\n",
      "0                       0   46.800000      Quito  Pichincha     D       13   \n",
      "1                       0   47.070000      Quito  Pichincha     D       13   \n",
      "2                       0   48.590000      Quito  Pichincha     D       13   \n",
      "3                       0   48.916095      Quito  Pichincha     D       13   \n",
      "4                       0   47.927188      Quito  Pichincha     D       13   \n",
      "...                   ...         ...        ...        ...   ...      ...   \n",
      "28507                   0   46.763127  El Carmen     Manabi     C        3   \n",
      "28508                   0   46.400000  El Carmen     Manabi     C        3   \n",
      "28509                   0   46.460000  El Carmen     Manabi     C        3   \n",
      "28510                   0   45.960000  El Carmen     Manabi     C        3   \n",
      "28511                   0   47.260000  El Carmen     Manabi     C        3   \n",
      "\n",
      "       transactions  \n",
      "0       1523.844272  \n",
      "1       1523.844272  \n",
      "2       1523.844272  \n",
      "3       1523.844272  \n",
      "4       1523.844272  \n",
      "...             ...  \n",
      "28507    865.924821  \n",
      "28508    865.924821  \n",
      "28509    865.924821  \n",
      "28510    865.924821  \n",
      "28511    865.924821  \n",
      "\n",
      "[28512 rows x 50 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For transactions in test period, we need to be careful\n",
    "# We can either:\n",
    "# 1. Use 0 for all test transactions (conservative)\n",
    "# 2. Use historical averages for each store\n",
    "# 3. Predict transactions separately\n",
    "\n",
    "# Option 2: Use historical averages\n",
    "store_avg_transactions = transactions_df.groupby('store_nbr')['transactions'].mean().reset_index()\n",
    "store_avg_transactions.columns = ['store_nbr', 'avg_transactions']\n",
    "\n",
    "# Merge with test data\n",
    "test_df_processed = test_df_processed.merge(store_avg_transactions, on='store_nbr', how='left')\n",
    "\n",
    "# Use average transactions for test period\n",
    "test_df_processed['transactions'] = test_df_processed['avg_transactions']\n",
    "test_df_processed.drop('avg_transactions', axis=1, inplace=True)\n",
    "\n",
    "print_null_rows(test_df_processed, 'test_df_transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03324482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in test data:\n",
      "id                           0\n",
      "date                         0\n",
      "store_nbr                    0\n",
      "family                       0\n",
      "sales                    28512\n",
      "onpromotion                  0\n",
      "year                         0\n",
      "month                        0\n",
      "day                          0\n",
      "dayofweek                    0\n",
      "weekofyear                   0\n",
      "day_of_year                  0\n",
      "is_weekend                   0\n",
      "is_month_start               0\n",
      "is_month_end                 0\n",
      "is_quarter_start             0\n",
      "is_quarter_end               0\n",
      "is_payday                    0\n",
      "days_since_payday            0\n",
      "days_until_payday            0\n",
      "sales_lag_1              26730\n",
      "sales_lag_7              16038\n",
      "sales_lag_14              3564\n",
      "sales_lag_30                 0\n",
      "sales_rolling_mean_7     17820\n",
      "sales_rolling_std_7      19602\n",
      "sales_rolling_max_7      17820\n",
      "sales_rolling_min_7      17820\n",
      "sales_rolling_mean_14     5346\n",
      "sales_rolling_std_14      7128\n",
      "sales_rolling_max_14      5346\n",
      "sales_rolling_min_14      5346\n",
      "sales_rolling_mean_30        0\n",
      "sales_rolling_std_30         0\n",
      "sales_rolling_max_30         0\n",
      "sales_rolling_min_30         0\n",
      "is_national_holiday          0\n",
      "is_regional_holiday          0\n",
      "is_local_holiday             0\n",
      "is_additional_holiday        0\n",
      "is_working_day               0\n",
      "is_event                     0\n",
      "is_bridge_day                0\n",
      "is_transferred_day           0\n",
      "dcoilwtico                   0\n",
      "city                         0\n",
      "state                        0\n",
      "type                         0\n",
      "cluster                      0\n",
      "transactions                 0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
      "C:\\Users\\Fady Adel\\AppData\\Local\\Temp\\ipykernel_11652\\2231596208.py:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values after filling:\n",
      "id                           0\n",
      "date                         0\n",
      "store_nbr                    0\n",
      "family                       0\n",
      "sales                    28512\n",
      "onpromotion                  0\n",
      "year                         0\n",
      "month                        0\n",
      "day                          0\n",
      "dayofweek                    0\n",
      "weekofyear                   0\n",
      "day_of_year                  0\n",
      "is_weekend                   0\n",
      "is_month_start               0\n",
      "is_month_end                 0\n",
      "is_quarter_start             0\n",
      "is_quarter_end               0\n",
      "is_payday                    0\n",
      "days_since_payday            0\n",
      "days_until_payday            0\n",
      "sales_lag_1                  0\n",
      "sales_lag_7                  0\n",
      "sales_lag_14                 0\n",
      "sales_lag_30                 0\n",
      "sales_rolling_mean_7         0\n",
      "sales_rolling_std_7          0\n",
      "sales_rolling_max_7          0\n",
      "sales_rolling_min_7          0\n",
      "sales_rolling_mean_14        0\n",
      "sales_rolling_std_14         0\n",
      "sales_rolling_max_14         0\n",
      "sales_rolling_min_14         0\n",
      "sales_rolling_mean_30        0\n",
      "sales_rolling_std_30         0\n",
      "sales_rolling_max_30         0\n",
      "sales_rolling_min_30         0\n",
      "is_national_holiday          0\n",
      "is_regional_holiday          0\n",
      "is_local_holiday             0\n",
      "is_additional_holiday        0\n",
      "is_working_day               0\n",
      "is_event                     0\n",
      "is_bridge_day                0\n",
      "is_transferred_day           0\n",
      "dcoilwtico                   0\n",
      "city                         0\n",
      "state                        0\n",
      "type                         0\n",
      "cluster                      0\n",
      "transactions                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for any remaining null values\n",
    "print(\"Null values in test data:\")\n",
    "print(test_df_processed.isnull().sum())\n",
    "\n",
    "# Fill any remaining nulls with appropriate values\n",
    "# For rolling features, use forward fill\n",
    "rolling_cols = [col for col in test_df_processed.columns if 'rolling' in col]\n",
    "for col in rolling_cols:\n",
    "    test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
    "\n",
    "# For lag features, use forward fill\n",
    "lag_cols = [col for col in test_df_processed.columns if 'lag' in col]\n",
    "for col in lag_cols:\n",
    "    test_df_processed[col] = test_df_processed.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
    "\n",
    "print(\"\\nNull values after filling:\")\n",
    "print(test_df_processed.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d6195c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop sales column from test data\n",
    "test_df_processed.drop('sales', axis=1, inplace=True, errors='ignore')\n",
    "# Save the processed test data\n",
    "test_df_processed.to_csv('../data/interim/traditional_final_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12327b",
   "metadata": {},
   "source": [
    "## 3. Handling Null Values for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_nulls_for_prediction(test_df, strategy='comprehensive'):\n",
    "    \"\"\"\n",
    "    Handle null values in test data for prediction using multiple strategies\n",
    "    \n",
    "    Parameters:\n",
    "    test_df: DataFrame with null values\n",
    "    strategy: 'forward_fill', 'group_mean', 'comprehensive'\n",
    "    \"\"\"\n",
    "    test_df_filled = test_df.copy()\n",
    "    \n",
    "    if strategy == 'forward_fill':\n",
    "        # Strategy 1: Forward fill within each store-family group\n",
    "        lag_cols = [col for col in test_df.columns if 'lag' in col]\n",
    "        rolling_cols = [col for col in test_df.columns if 'rolling' in col]\n",
    "        \n",
    "        for col in lag_cols + rolling_cols:\n",
    "            test_df_filled[col] = test_df_filled.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
    "        \n",
    "        # Fill any remaining nulls with 0\n",
    "        test_df_filled[lag_cols + rolling_cols] = test_df_filled[lag_cols + rolling_cols].fillna(0)\n",
    "    \n",
    "    elif strategy == 'group_mean':\n",
    "        # Strategy 2: Use historical averages from training data\n",
    "        # This requires passing training data statistics\n",
    "        pass  # Implement if training stats are available\n",
    "    \n",
    "    elif strategy == 'comprehensive':\n",
    "        # Strategy 3: Comprehensive approach (recommended)\n",
    "        \n",
    "        # 1. Forward fill lag features within groups\n",
    "        lag_cols = [col for col in test_df.columns if 'lag' in col]\n",
    "        for col in lag_cols:\n",
    "            test_df_filled[col] = test_df_filled.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
    "        \n",
    "        # 2. Forward fill rolling features within groups\n",
    "        rolling_cols = [col for col in test_df.columns if 'rolling' in col]\n",
    "        for col in rolling_cols:\n",
    "            test_df_filled[col] = test_df_filled.groupby(['store_nbr', 'family'])[col].fillna(method='ffill')\n",
    "        \n",
    "        # 3. For remaining nulls in lag features, use the last available value or 0\n",
    "        for col in lag_cols:\n",
    "            # Use median of available values for each store-family, then 0\n",
    "            group_medians = test_df_filled.groupby(['store_nbr', 'family'])[col].transform('median')\n",
    "            test_df_filled[col] = test_df_filled[col].fillna(group_medians).fillna(0)\n",
    "        \n",
    "        # 4. For remaining nulls in rolling features, use available values or 0\n",
    "        for col in rolling_cols:\n",
    "            if 'std' in col:\n",
    "                # For standard deviation, use 0 for missing values\n",
    "                test_df_filled[col] = test_df_filled[col].fillna(0)\n",
    "            else:\n",
    "                # For mean, max, min use group median then 0\n",
    "                group_medians = test_df_filled.groupby(['store_nbr', 'family'])[col].transform('median')\n",
    "                test_df_filled[col] = test_df_filled[col].fillna(group_medians).fillna(0)\n",
    "    \n",
    "    return test_df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the comprehensive null handling strategy\n",
    "test_df_final = handle_nulls_for_prediction(test_df_processed, strategy='comprehensive')\n",
    "\n",
    "print(\"Null values before handling:\")\n",
    "print(test_df_processed.isnull().sum().sum())\n",
    "\n",
    "print(\"\\nNull values after handling:\")\n",
    "print(test_df_final.isnull().sum().sum())\n",
    "\n",
    "print(\"\\nDetailed null counts after handling:\")\n",
    "null_counts = test_df_final.isnull().sum()\n",
    "print(null_counts[null_counts > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0308b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Create indicator features for missing values\n",
    "def create_missing_indicators(df):\n",
    "    \"\"\"Create binary indicators for originally missing values\"\"\"\n",
    "    df_with_indicators = df.copy()\n",
    "    \n",
    "    # Create indicators for lag features\n",
    "    lag_cols = [col for col in df.columns if 'lag' in col]\n",
    "    for col in lag_cols:\n",
    "        df_with_indicators[f'{col}_missing'] = df[col].isnull().astype(int)\n",
    "    \n",
    "    # Create indicators for rolling features\n",
    "    rolling_cols = [col for col in df.columns if 'rolling' in col and 'std' not in col]\n",
    "    for col in rolling_cols:\n",
    "        df_with_indicators[f'{col}_missing'] = df[col].isnull().astype(int)\n",
    "    \n",
    "    return df_with_indicators\n",
    "\n",
    "# Create missing indicators before filling nulls\n",
    "test_df_with_indicators = create_missing_indicators(test_df_processed)\n",
    "test_df_with_indicators = handle_nulls_for_prediction(test_df_with_indicators, strategy='comprehensive')\n",
    "\n",
    "print(f\"Test data shape with missing indicators: {test_df_with_indicators.shape}\")\n",
    "print(f\"Original shape: {test_df_processed.shape}\")\n",
    "print(f\"Added {test_df_with_indicators.shape[1] - test_df_processed.shape[1]} indicator features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-specific strategies\n",
    "def prepare_for_different_models(df):\n",
    "    \"\"\"Prepare data differently based on model type\"\"\"\n",
    "    \n",
    "    # For Tree-based models (Random Forest, XGBoost, etc.)\n",
    "    df_tree = df.copy()\n",
    "    # Tree models can handle some nulls, but let's fill them\n",
    "    df_tree = handle_nulls_for_prediction(df_tree, strategy='comprehensive')\n",
    "    \n",
    "    # For Linear models (Linear Regression, Lasso, etc.)\n",
    "    df_linear = df.copy()\n",
    "    # Linear models need all nulls filled and may benefit from indicators\n",
    "    df_linear = create_missing_indicators(df_linear)\n",
    "    df_linear = handle_nulls_for_prediction(df_linear, strategy='comprehensive')\n",
    "    \n",
    "    # For Neural Networks\n",
    "    df_nn = df_linear.copy()  # Same as linear but may need scaling\n",
    "    \n",
    "    return {\n",
    "        'tree_models': df_tree,\n",
    "        'linear_models': df_linear,\n",
    "        'neural_networks': df_nn\n",
    "    }\n",
    "\n",
    "# Prepare data for different model types\n",
    "model_ready_data = prepare_for_different_models(test_df_processed)\n",
    "\n",
    "for model_type, data in model_ready_data.items():\n",
    "    print(f\"{model_type}: {data.shape}, nulls: {data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed test data for prediction\n",
    "test_df_final.to_csv('../data/interim/traditional_final_test_filled.csv', index=False)\n",
    "test_df_with_indicators.to_csv('../data/interim/traditional_final_test_with_indicators.csv', index=False)\n",
    "\n",
    "print(\"Saved processed test data files:\")\n",
    "print(\"1. traditional_final_test_filled.csv - Basic null handling\")\n",
    "print(\"2. traditional_final_test_with_indicators.csv - With missing value indicators\")\n",
    "\n",
    "# Final verification\n",
    "print(f\"\\nFinal test data shape: {test_df_final.shape}\")\n",
    "print(f\"Final null count: {test_df_final.isnull().sum().sum()}\")\n",
    "print(f\"Ready for prediction: {test_df_final.isnull().sum().sum() == 0}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
